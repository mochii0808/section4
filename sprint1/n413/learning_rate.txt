## 신경망 성능 향상 ##

1. 학습률 조정

1) 학습률(lr, η)
- 경사 하강법을 이용하여 구한 기울기를 얼마나 사용할지 결정

2) 학습률 감소
- 학습률이 감소하는 정도 제한
- 옵티마이저 파라미터 조정

3) 학습률 계획법
- 학습률을 유동적으로 사용할 수 있게 설정
    - warm-up step을 준 뒤 천천히 하강
- .experimental 내부의 함수 사용

---------------------------------------------------------------------------

2. 가중치 초기화

1) 가중치 초기화 이유
- 가중치 설정에 따라 기울기 소실 문제가 발생하며, 또 해결할 수도 있다.
- 같은 모델이라도 초기 가중치에 따라 훈련 결과가 달라짐

2) 표준편차 1인 정규분포
- 활성화 값이 대부분 0, 1
    - 신경망의 가중치가 모두 같다
    -> 갱신 후 가중치도 모두 같다
    -> 노드를 여러 개 두는 의미가 없다

3) Xavier 초기화
- 이전 층 노드가 n개일 때, 현재 층의 가중치를 표준편차가 1/√n인 정규분포로 초기화
- 시그모이드

4) He 초기화
- 이전 층 노드가 n개일 때, 현재 층의 가중치를 표준편차가 2/√n인 정규분포로 초기화
- 렐루

-----------------------------------------------------------------------------

3. 과적합 방지

1) Weight Decay(가중치 감소)
- 손실 함수에 가중치 규제항을 추가
- L1, L2
    - L1 : 부호 반대 상수
    - L2 : (1-η) 비율
- regularizers.l1(0.01)

2) Dropout
- 일정 비율의 노드 0으로
    - 노드마다 확률적으로 0이 됨
- 적용하려는 층에 Dropout 함수 추가

3) Early Stopping
- 검증 데이터의 손실이 증가하는 시점에 학습 중지