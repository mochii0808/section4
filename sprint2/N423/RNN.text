언어 모델
- 단어 시퀀스(문장)에서 각 단어의 확률을 계산하는 모델

통계적 언어 모델(SLM)
- 단어 등장 횟수 바탕으로 조건부 확률 계산
- 희소성 문제 : 학습데이터에 없는 단어는 생성 불가
- 개선 방법 : N-gram, smoothing, back-off

신경망 언어 모델(NLM)
- 횟수 기반이 아닌 임베딩 벡터 사용

----------------------------------------------------------------------------------------------------------


RNN(순환 신경망)
- 출력 벡터가 다시 입력되는 특성
    - 이전 벡터가 다음층(시점)으로 계속 입력
    - 모든 이전 시점의 정보 획득
    - 시계열 데이터 처리
- 모든 벡터 순차적 입력 -> 병렬화 불가 -> GPU연산 불가
- 기울기 폭발/소실
- 10~20 개 규모의 단어일 때 사용

----------------------------------------------------------------------------------------------------------


LSTM(장단기 기억망)
- RNN에 기울기 크기 조절을 위한 Gate추가
    - forget : 과거 정보를 얼마나 사용할지
    - input : 현재 정보는 얼마나 사용할지 
    - output : 두 정보를 얼마나 넘길지
- cell state 추가
    - 역전파 과정에 활성화 함수가 없어 정보 손실 X


GRU
- LSTM 간소화
- cell state 삭제
    - c 와 h 통합
    - 과거 정보 + 현재 정보 = 1

----------------------------------------------------------------------------------------------------------


RNN 기반의 단점
- 장기 의존성 : 문장이 길어지면 앞 단어의 정보 소실
- 고정된 context vector로 인해 정보량 한계

Attention
- 입력 단어만큼의 hidden state벡터 저장
- 쿼리 : 디코더 hidden state 벡터
- 키, 밸류 : 인코더 hidden state 벡터

Attention 과정
1. 쿼리, 키 내적
2. 소프트 맥스
3. 1.결과에 밸류 곱
4. 3.결과 총합산
=> 쿼리-키 연관성이 높은 벡터 성분 높음